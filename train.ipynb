{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow_federated as tff\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            down         up  rnti_count   mcs_down  \\\n",
      "2018-03-28 15:56:00  174876888.0  1856888.0       10229  15.332298   \n",
      "2018-03-28 15:58:00  209054184.0  2866200.0       12223  15.116846   \n",
      "2018-03-28 16:00:00  191464640.0  1935360.0       11152  15.215739   \n",
      "2018-03-28 16:02:00  241515688.0  2991152.0       14040  15.135400   \n",
      "2018-03-28 16:04:00  264131088.0  3288816.0       15247  15.188944   \n",
      "...                          ...        ...         ...        ...   \n",
      "2018-04-03 11:30:00  128110784.0  1088280.0        7758  14.980204   \n",
      "2018-04-03 11:32:00  151273208.0  1283992.0        9103  15.013859   \n",
      "2018-04-03 11:34:00  149891584.0  1222184.0        8923  15.202415   \n",
      "2018-04-03 11:36:00  155997488.0  1331536.0        9309  15.051294   \n",
      "2018-04-03 11:38:00  172544880.0  1846208.0       10338  15.219135   \n",
      "\n",
      "                     mcs_down_var     mcs_up  mcs_up_var   rb_down  \\\n",
      "2018-03-28 15:56:00     87.157688  14.981497   49.989484  0.029681   \n",
      "2018-03-28 15:58:00     87.192168  16.432612   62.494670  0.035971   \n",
      "2018-03-28 16:00:00     87.227955  15.885238   63.087007  0.032750   \n",
      "2018-03-28 16:02:00     86.199501  15.714660   77.187459  0.041372   \n",
      "2018-03-28 16:04:00     86.151119  15.414080   69.118561  0.045074   \n",
      "...                           ...        ...         ...       ...   \n",
      "2018-04-03 11:30:00     86.783498  14.489431   35.620732  0.021526   \n",
      "2018-04-03 11:32:00     87.903331  16.019702   42.272334  0.025785   \n",
      "2018-04-03 11:34:00     86.370911  16.476345   34.919566  0.025406   \n",
      "2018-04-03 11:36:00     87.701243  15.943630   51.888593  0.026372   \n",
      "2018-04-03 11:38:00     87.745979  16.592670   52.344327  0.029585   \n",
      "\n",
      "                      rb_down_var     rb_up     rb_up_var  \n",
      "2018-03-28 15:56:00  4.497698e-08  0.000541  3.143297e-08  \n",
      "2018-03-28 15:58:00  4.615535e-08  0.000852  4.439640e-08  \n",
      "2018-03-28 16:00:00  4.646104e-08  0.000607  2.993595e-08  \n",
      "2018-03-28 16:02:00  4.532153e-08  0.000925  5.382563e-08  \n",
      "2018-03-28 16:04:00  4.655543e-08  0.001021  5.922178e-08  \n",
      "...                           ...       ...           ...  \n",
      "2018-04-03 11:30:00  4.404124e-08  0.000380  1.945518e-08  \n",
      "2018-04-03 11:32:00  4.499877e-08  0.000444  2.165493e-08  \n",
      "2018-04-03 11:34:00  4.622009e-08  0.000373  1.820744e-08  \n",
      "2018-04-03 11:36:00  4.449000e-08  0.000388  1.931032e-08  \n",
      "2018-04-03 11:38:00  4.505553e-08  0.000563  3.314203e-08  \n",
      "\n",
      "[4192 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "train_dbfile = open('train.pkl', 'rb')     \n",
    "train_dataset = pickle.load(train_dbfile)\n",
    "test_dbfile = open('test.pkl', 'rb')     \n",
    "test_dataset = pickle.load(test_dbfile)\n",
    "\n",
    "Client_1 = train_dataset['ElBorn']\n",
    "Client_2 = train_dataset['LesCorts']\n",
    "Client_3 = train_dataset['PobleSec']\n",
    "\n",
    "print(Client_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'snippets': 'Yonder comes my master, your brother.'}, {'snippets': 'But do not so. I have five hundred crowns,'}, {'snippets': 'I scarce can speak to thank you for myself.\\n'}, {'snippets': \"[Coming forward] Sweet masters, be patient; for your father's\"}, {'snippets': \"remembrance, be at accord.\\nIs 'old dog' my reward? Most true, I have lost my teeth in\"}, {'snippets': 'Come not within these doors; within this roof\\nThe enemy of all your graces lives.\\nYour brother- no, no brother; yet the son-\\nYet not the son; I will not call him son\\nOf him I was about to call his father-\\nHath heard your praises; and this night he means\\nTo burn the lodging where you use to lie,\\nAnd you within it. If he fail of that,\\nHe will have other means to cut you off;\\nI overheard him and his practices.\\nThis is no place; this house is but a butchery;\\nAbhor it, fear it, do not enter it.\\nNo matter whither, so you come not here.'}, {'snippets': 'O my sweet master! O you memory\\nOf old Sir Rowland! Why, what make you here?\\nWhy are you virtuous? Why do people love you?\\nAnd wherefore are you gentle, strong, and valiant?\\nWhy would you be so fond to overcome\\nThe bonny prizer of the humorous Duke?\\nYour praise is come too swiftly home before you.\\nKnow you not, master, to some kind of men\\nTheir graces serve them but as enemies?\\nNo more do yours. Your virtues, gentle master,\\nAre sanctified and holy traitors to you.\\nO, what a world is this, when what is comely\\nEnvenoms him that bears it!\\nO unhappy youth!'}, {'snippets': \"The thrifty hire I sav'd under your father,\\nWhich I did store to be my foster-nurse,\\nWhen service should in my old limbs lie lame,\\nAnd unregarded age in corners thrown.\\nTake that, and He that doth the ravens feed,\\nYea, providently caters for the sparrow,\\nBe comfort to my age! Here is the gold;\\nAll this I give you. Let me be your servant;\\nThough I look old, yet I am strong and lusty;\\nFor in my youth I never did apply\\nHot and rebellious liquors in my blood,\\nNor did not with unbashful forehead woo\\nThe means of weakness and debility;\\nTherefore my age is as a lusty winter,\\nFrosty, but kindly. Let me go with you;\\nI'll do the service of a younger man\\nIn all your business and necessities.\\nMaster, go on; and I will follow the\"}, {'snippets': \"To the last gasp, with truth and loyalty.\\nFrom seventeen years till now almost four-score\\nHere lived I, but now live here no more.\\nAt seventeen years many their fortunes seek,\\nBut at fourscore it is too late a week;\\nYet fortune cannot recompense me better\\nThan to die well and not my master's debtor.          Exeunt\\nDear master, I can go no further. O, I die for food! Here lie\"}]\n"
     ]
    }
   ],
   "source": [
    "client_id_colname = 'character' # the column that represents client ID\n",
    "data_colname = 'snippets'\n",
    "def create_tff_dataset_for_csv_file(filepath):\n",
    "  df = pd.read_csv(filepath)\n",
    "  client_ids = df[client_id_colname].unique().tolist()\n",
    "  # print(client_ids)\n",
    "\n",
    "  def create_tf_dataset_for_client_fn(client_id):                       # helper Function that takes the client ids then filter out any null values\n",
    "    client_data = df[df[client_id_colname] == client_id]                    # Then selects only the data i.e the snippets\n",
    "    client_data = client_data[client_data[data_colname].notnull()]             # Then returns records dict with client id as key\n",
    "    client_data = client_data[[data_colname]]\n",
    "    records = client_data.to_dict('records')\n",
    "    # print(client_data)\n",
    "    print(records)\n",
    "\n",
    "    def dataset_gen():\n",
    "      for row in records:\n",
    "        yield row\n",
    "    return tf.data.Dataset.from_generator(dataset_gen,\n",
    "                                          output_types={data_colname: tf.string},\n",
    "                                          output_shapes={data_colname: []}\n",
    "    )\n",
    "\n",
    "  return tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "      client_ids=client_ids,\n",
    "      serializable_dataset_fn=create_tf_dataset_for_client_fn\n",
    "  )\n",
    "\n",
    "filepath = 'shakespeare_train.csv'\n",
    "train_data = create_tff_dataset_for_csv_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_colname = [\"down\",\"up\",\"rnti_count\"]\n",
    "def create_tff_dataset_for_csv_file(filepath):\n",
    "  df = pd.read_pickle('train.pkl')   \n",
    "  client_ids = ['ElBorn','LesCorts','PobleSec']\n",
    "\n",
    "  def create_tf_dataset_for_client_fn(client_id):                       # helper Function that takes the client ids then filter out any null values\n",
    "    client_data = df[client_id]                    # Then selects only the data i.e the snippets\n",
    "    client_data = client_data[data_colname]\n",
    "    records = client_data.to_dict('records')\n",
    "\n",
    "    def dataset_gen():\n",
    "      for row in records:\n",
    "        yield row\n",
    "    return tf.data.Dataset.from_generator(dataset_gen,\n",
    "                                          output_types={data_colname: tf.string},\n",
    "                                          output_shapes={data_colname: []}\n",
    "    )\n",
    "\n",
    "  return tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "      client_ids=client_ids,\n",
    "      serializable_dataset_fn=create_tf_dataset_for_client_fn\n",
    "  )\n",
    "\n",
    "filepath = 'train.pkl'\n",
    "train_data = create_tff_dataset_for_csv_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>down</th>\n",
       "      <th>up</th>\n",
       "      <th>rnti_count</th>\n",
       "      <th>mcs_down</th>\n",
       "      <th>mcs_down_var</th>\n",
       "      <th>mcs_up</th>\n",
       "      <th>mcs_up_var</th>\n",
       "      <th>rb_down</th>\n",
       "      <th>rb_down_var</th>\n",
       "      <th>rb_up</th>\n",
       "      <th>rb_up_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:12:00</th>\n",
       "      <td>38490328.0</td>\n",
       "      <td>18729576.0</td>\n",
       "      <td>4495</td>\n",
       "      <td>2.573617</td>\n",
       "      <td>45.616769</td>\n",
       "      <td>13.382755</td>\n",
       "      <td>71.070384</td>\n",
       "      <td>0.013610</td>\n",
       "      <td>1.961025e-08</td>\n",
       "      <td>0.008343</td>\n",
       "      <td>1.083682e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:14:00</th>\n",
       "      <td>28093304.0</td>\n",
       "      <td>19340160.0</td>\n",
       "      <td>4277</td>\n",
       "      <td>2.104966</td>\n",
       "      <td>38.916538</td>\n",
       "      <td>14.779940</td>\n",
       "      <td>75.957494</td>\n",
       "      <td>0.012278</td>\n",
       "      <td>1.644211e-08</td>\n",
       "      <td>0.007040</td>\n",
       "      <td>1.061491e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:16:00</th>\n",
       "      <td>32971216.0</td>\n",
       "      <td>20630232.0</td>\n",
       "      <td>4729</td>\n",
       "      <td>2.452463</td>\n",
       "      <td>45.055805</td>\n",
       "      <td>14.624951</td>\n",
       "      <td>72.153009</td>\n",
       "      <td>0.013160</td>\n",
       "      <td>1.761309e-08</td>\n",
       "      <td>0.009001</td>\n",
       "      <td>9.109703e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:18:00</th>\n",
       "      <td>28745776.0</td>\n",
       "      <td>13323504.0</td>\n",
       "      <td>3924</td>\n",
       "      <td>2.072077</td>\n",
       "      <td>37.965933</td>\n",
       "      <td>13.979430</td>\n",
       "      <td>71.980549</td>\n",
       "      <td>0.012282</td>\n",
       "      <td>1.668480e-08</td>\n",
       "      <td>0.006028</td>\n",
       "      <td>5.813802e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:20:00</th>\n",
       "      <td>30887688.0</td>\n",
       "      <td>13812144.0</td>\n",
       "      <td>4099</td>\n",
       "      <td>2.241337</td>\n",
       "      <td>40.990956</td>\n",
       "      <td>13.314127</td>\n",
       "      <td>77.085867</td>\n",
       "      <td>0.012741</td>\n",
       "      <td>1.721707e-08</td>\n",
       "      <td>0.006706</td>\n",
       "      <td>7.246522e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22 06:46:00</th>\n",
       "      <td>48370808.0</td>\n",
       "      <td>231664.0</td>\n",
       "      <td>2929</td>\n",
       "      <td>15.198009</td>\n",
       "      <td>85.244211</td>\n",
       "      <td>14.277778</td>\n",
       "      <td>25.174603</td>\n",
       "      <td>0.008044</td>\n",
       "      <td>4.418387e-08</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>3.346528e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22 06:48:00</th>\n",
       "      <td>44972096.0</td>\n",
       "      <td>184784.0</td>\n",
       "      <td>2847</td>\n",
       "      <td>15.449166</td>\n",
       "      <td>87.724101</td>\n",
       "      <td>14.988889</td>\n",
       "      <td>24.066667</td>\n",
       "      <td>0.007614</td>\n",
       "      <td>4.375022e-08</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>3.466806e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22 06:50:00</th>\n",
       "      <td>31460120.0</td>\n",
       "      <td>33088.0</td>\n",
       "      <td>1997</td>\n",
       "      <td>15.330073</td>\n",
       "      <td>86.727797</td>\n",
       "      <td>12.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>4.269245e-08</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22 06:52:00</th>\n",
       "      <td>28048016.0</td>\n",
       "      <td>48072.0</td>\n",
       "      <td>1748</td>\n",
       "      <td>15.351619</td>\n",
       "      <td>92.460379</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.004745</td>\n",
       "      <td>4.644352e-08</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>2.041667e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22 06:54:00</th>\n",
       "      <td>41325736.0</td>\n",
       "      <td>158248.0</td>\n",
       "      <td>2563</td>\n",
       "      <td>15.173839</td>\n",
       "      <td>89.072241</td>\n",
       "      <td>16.550000</td>\n",
       "      <td>36.650000</td>\n",
       "      <td>0.006851</td>\n",
       "      <td>4.615977e-08</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>4.252500e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6892 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           down          up  rnti_count   mcs_down  \\\n",
       "2019-01-12 17:12:00  38490328.0  18729576.0        4495   2.573617   \n",
       "2019-01-12 17:14:00  28093304.0  19340160.0        4277   2.104966   \n",
       "2019-01-12 17:16:00  32971216.0  20630232.0        4729   2.452463   \n",
       "2019-01-12 17:18:00  28745776.0  13323504.0        3924   2.072077   \n",
       "2019-01-12 17:20:00  30887688.0  13812144.0        4099   2.241337   \n",
       "...                         ...         ...         ...        ...   \n",
       "2019-01-22 06:46:00  48370808.0    231664.0        2929  15.198009   \n",
       "2019-01-22 06:48:00  44972096.0    184784.0        2847  15.449166   \n",
       "2019-01-22 06:50:00  31460120.0     33088.0        1997  15.330073   \n",
       "2019-01-22 06:52:00  28048016.0     48072.0        1748  15.351619   \n",
       "2019-01-22 06:54:00  41325736.0    158248.0        2563  15.173839   \n",
       "\n",
       "                     mcs_down_var     mcs_up  mcs_up_var   rb_down  \\\n",
       "2019-01-12 17:12:00     45.616769  13.382755   71.070384  0.013610   \n",
       "2019-01-12 17:14:00     38.916538  14.779940   75.957494  0.012278   \n",
       "2019-01-12 17:16:00     45.055805  14.624951   72.153009  0.013160   \n",
       "2019-01-12 17:18:00     37.965933  13.979430   71.980549  0.012282   \n",
       "2019-01-12 17:20:00     40.990956  13.314127   77.085867  0.012741   \n",
       "...                           ...        ...         ...       ...   \n",
       "2019-01-22 06:46:00     85.244211  14.277778   25.174603  0.008044   \n",
       "2019-01-22 06:48:00     87.724101  14.988889   24.066667  0.007614   \n",
       "2019-01-22 06:50:00     86.727797  12.111111    0.000000  0.005174   \n",
       "2019-01-22 06:52:00     92.460379   7.900000    1.800000  0.004745   \n",
       "2019-01-22 06:54:00     89.072241  16.550000   36.650000  0.006851   \n",
       "\n",
       "                      rb_down_var     rb_up     rb_up_var  \n",
       "2019-01-12 17:12:00  1.961025e-08  0.008343  1.083682e-08  \n",
       "2019-01-12 17:14:00  1.644211e-08  0.007040  1.061491e-08  \n",
       "2019-01-12 17:16:00  1.761309e-08  0.009001  9.109703e-09  \n",
       "2019-01-12 17:18:00  1.668480e-08  0.006028  5.813802e-09  \n",
       "2019-01-12 17:20:00  1.721707e-08  0.006706  7.246522e-09  \n",
       "...                           ...       ...           ...  \n",
       "2019-01-22 06:46:00  4.418387e-08  0.000096  3.346528e-09  \n",
       "2019-01-22 06:48:00  4.375022e-08  0.000056  3.466806e-09  \n",
       "2019-01-22 06:50:00  4.269245e-08  0.000014  0.000000e+00  \n",
       "2019-01-22 06:52:00  4.644352e-08  0.000025  2.041667e-09  \n",
       "2019-01-22 06:54:00  4.615977e-08  0.000069  4.252500e-09  \n",
       "\n",
       "[6892 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"./dummy.pkl\")\n",
    "# [\"down\",\"up\",\"rnti_count\",\"mcs_down\",\"mcs_down_var\",\"mcs_up\",\"mcs_up_var\",\"rb_down\",\"rb_down_var\",\"rb_up\",\"rb_up_var\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow import reshape, nest, config\n",
    "import tensorflow_federated as tff\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.layers import Input, GRU, Dense, Flatten, Reshape, BatchNormalization, LayerNormalization, Conv2D, \\\n",
    "    SeparableConv2D, AveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError as MSE, MeanAbsoluteError as MAE\n",
    "from keras.metrics import MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dbfile = open('train.pkl', 'rb')     \n",
    "train_dataset = pickle.load(train_dbfile)\n",
    "test_dbfile = open('test.pkl', 'rb')     \n",
    "test_dataset = pickle.load(test_dbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Total number of samples:  4192\n",
      "- Number of samples for training: 3354\n",
      "- Number of samples for validation:  838\n",
      "-------------Done for ElBorn--------------------\n",
      "- Total number of samples:  6892\n",
      "- Number of samples for training: 5514\n",
      "- Number of samples for validation:  1378\n",
      "-------------Done for LesCorts--------------------\n",
      "- Total number of samples:  15927\n",
      "- Number of samples for training: 12742\n",
      "- Number of samples for validation:  3185\n",
      "-------------Done for PobleSec--------------------\n"
     ]
    }
   ],
   "source": [
    "train_dbfile = open('train.pkl', 'rb')     \n",
    "train_dataset = pickle.load(train_dbfile)\n",
    "client_names = ['ElBorn','LesCorts','PobleSec']\n",
    "\n",
    "def dat(train_dataset,client_name):\n",
    "    # Load the data from the training dataset\n",
    "    processed_data = train_dataset.get(client_name).loc[:,['rnti_count', 'rb_down', 'rb_up', 'mcs_down', 'mcs_up', 'down', 'up']].astype('float32').fillna(0)\n",
    "\n",
    "    # Split data into training and validation\n",
    "    train_percentage = 0.8\n",
    "    num_samples_train = math.ceil(train_percentage*len(processed_data))\n",
    "    print(f\"- Total number of samples:  {len(processed_data)}\")\n",
    "    print(f\"- Number of samples for training: {num_samples_train}\")\n",
    "    print(f\"- Number of samples for validation:  {len(processed_data)-num_samples_train}\")\n",
    "    print(f\"-------------Done for {client_name}--------------------\")\n",
    "    train_data = processed_data[:num_samples_train]\n",
    "    val_data = processed_data[num_samples_train:]\n",
    "\n",
    "    scalerX = MinMaxScaler(feature_range=(-1,1))\n",
    "    scalerY = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "    # numpy arrays from here on\n",
    "    X_train = scalerX.fit_transform(train_data.loc[:,['rb_down', 'rb_up', 'rnti_count', 'mcs_down', 'mcs_up']])\n",
    "    X_train = X_train[..., np.newaxis] # CNN requested shape\n",
    "    X_val = scalerX.transform(val_data.loc[:,['rb_down', 'rb_up', 'rnti_count', 'mcs_down', 'mcs_up']])\n",
    "    X_val = X_val[..., np.newaxis] # CNN requested shape\n",
    "    Y_train = scalerY.fit_transform(train_data.loc[:,['rnti_count', 'rb_down', 'rb_up', 'down', 'up']])\n",
    "    Y_val = scalerY.transform(val_data.loc[:,['rnti_count', 'rb_down', 'rb_up', 'down', 'up']])\n",
    "\n",
    "    return X_train,Y_train,X_val,Y_val\n",
    "\n",
    "\n",
    "client_train_dataset = collections.OrderedDict()\n",
    "for i in range(3):\n",
    "    client_name = client_names[i]\n",
    "    x_train,y_train,x_val,y_val = dat(train_dataset,client_name)\n",
    "    data = collections.OrderedDict((('label', y_train), ('feat', x_train)))\n",
    "    client_train_dataset[client_name] = data\n",
    "\n",
    "train_dataset = tff.simulation.datasets.TestClientData(client_train_dataset)\n",
    "sample_dataset = train_dataset.create_tf_dataset_for_client(train_dataset.client_ids[0])\n",
    "sample_element = next(iter(sample_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROUNDS = 5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 20\n",
    "PREFETCH_BUFFER = 10\n",
    "SHUFFLE_BUFFER = 1000\n",
    "\n",
    "def preprocess(dataset):\n",
    "\n",
    "  def batch_format_fn(element):\n",
    "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
    "\n",
    "    return collections.OrderedDict(\n",
    "        x=element['feat'],\n",
    "        y=element['label'])\n",
    "\n",
    "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(\n",
    "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
    "\n",
    "preprocessed_sample_dataset = preprocess(sample_dataset)\n",
    "sample_batch = nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_sample_dataset)))\n",
    "\n",
    "\n",
    "def make_federated_data(client_data, client_ids):\n",
    "    return [preprocess(client_data.create_tf_dataset_for_client(x)) for x in client_ids]\n",
    "\n",
    "federated_train_data = make_federated_data(train_dataset, train_dataset.client_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of client datasets: 3\n",
      "First dataset: <PrefetchDataset element_spec=OrderedDict([('x', TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name=None)), ('y', TensorSpec(shape=(None, 5), dtype=tf.float32, name=None))])>\n"
     ]
    }
   ],
   "source": [
    "print('Number of client datasets: {l}'.format(l=len(federated_train_data)))\n",
    "print('First dataset: {d}'.format(d=federated_train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(inputShape, nVarsOut, nFuture=1, nFilters=16, kernelSize1=[16, 3], kernelSize2=[3, 5], kernelSize3=[8, 3],\n",
    "              kernelSize4=[4, 3], poolSize=[2, 1], mask='11111'):\n",
    "  \n",
    "    model = Sequential([\n",
    "        Conv2D(filters=nFilters, kernel_size=kernelSize1, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[0])), name='Conv2D_1',\n",
    "               input_shape=inputShape),\n",
    "        Conv2D(filters=nFilters, kernel_size=kernelSize2, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[1])), name='Conv2D_2'),\n",
    "        Conv2D(filters=nFilters * 2, kernel_size=kernelSize3, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[2])), name='Conv2D_3'),\n",
    "        Conv2D(filters=nFilters * 2, kernel_size=kernelSize4, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[3])), name='Conv2D_4'),\n",
    "        AveragePooling2D(pool_size=poolSize, name='AveragePooling'),\n",
    "        Flatten(name='flatten'),\n",
    "        Dense(nFuture * nVarsOut, activation='tanh', name='forecast', trainable=bool(int(mask[4])))  # [-1, 1]\n",
    "    ],\n",
    "        name=f'CNN_{nPast}to{nFuture}_{nFilters}-{kernelSize1[0]}x{kernelSize1[1]}_{nFilters}-{kernelSize2[0]}x{kernelSize2[1]}_{nFilters * 2}-{kernelSize3[0]}x{kernelSize3[1]}_{nFilters * 2}-{kernelSize4[0]}x{kernelSize4[1]}_finalPooling{poolSize[0]}x{poolSize[1]}_' + mask\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss=MSE(),\n",
    "                  metrics=[MeanAbsoluteError()])\n",
    "\n",
    "    print(model.summary(), flush=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPast = 10                  # Number of past samples used for predictions (equivalent to 20 minutes)\n",
    "nFuture = 1                 # Number of future samples to be predicted (equivalent to 2 minutes)\n",
    "windowSize = 1              # Window size (time-series forecasting)\n",
    "nEpochs = 10                # Number of epochs\n",
    "batch_size = 128            # Batch size (other options: [64, 32])\n",
    "nRep = 1                    # Number of repetitions whereby the model will be trained\n",
    "\n",
    "nVarsOut = y_val.shape[1]             # Output shape\n",
    "inputShape = nPast, x_val.shape[1], 1 # Input shape based on the number of past observations and the shape of the validation data\n",
    "\n",
    "loss1 = MSE(reduction='sum')\n",
    "loss2 = MAE(reduction='sum')\n",
    "\n",
    "model = CNN(inputShape, nVarsOut, nFuture)\n",
    "# Define the train and validation time-series\n",
    "genValLosses = TimeseriesGenerator(x_val, y_val, nPast, stride=windowSize, batch_size=1)  # BS always 1 here!\n",
    "genTrain = TimeseriesGenerator(x_train, y_train, nPast, stride=windowSize, batch_size=batch_size)\n",
    "genVal = TimeseriesGenerator(x_val, y_val, nPast, stride=windowSize, batch_size=batch_size)\n",
    "\n",
    "# Define arrays to store performance metrics \n",
    "avg = {'loss': np.zeros(nEpochs), 'val_loss': np.zeros(nEpochs), 'mean_absolute_error': np.zeros(nEpochs),\n",
    "        'val_mean_absolute_error': np.zeros(nEpochs)}\n",
    "\n",
    "# Train the model for nRep repetitions\n",
    "for i in range(nRep):                \n",
    "    history = model.fit(genTrain, epochs=nEpochs, validation_data=genVal, verbose=2)\n",
    "    avg['loss'] += np.array(history.history['loss'])\n",
    "    avg['val_loss'] += np.array(history.history['val_loss'])\n",
    "    avg['mean_absolute_error'] += np.array(history.history['mean_absolute_error'])\n",
    "    avg['val_mean_absolute_error'] += np.array(history.history['val_mean_absolute_error'])\n",
    "\n",
    "# Compute the average performance obtained across nRep repetitions\n",
    "avg = {k: v / nRep for k, v in avg.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 11:33:11.248590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1614] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13244 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:3b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fed Dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 23:43:57.872539: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-29 23:43:58.016003: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-29 23:43:58.055213: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-29 23:43:58.631565: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-29 23:43:58.631642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-29 23:43:58.631651: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow import reshape, nest, config\n",
    "import tensorflow_federated as tff\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.layers import Input, GRU, Dense, Flatten, Reshape, BatchNormalization, LayerNormalization, Conv2D, \\\n",
    "    SeparableConv2D, AveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError as MSE, MeanAbsoluteError as MAE\n",
    "from keras.metrics import MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Total number of samples:  4192\n",
      "- Number of samples for training: 3354\n",
      "- Number of samples for validation:  838\n",
      "-------------Done for ElBorn--------------------\n",
      "- Total number of samples:  6892\n",
      "- Number of samples for training: 5514\n",
      "- Number of samples for validation:  1378\n",
      "-------------Done for LesCorts--------------------\n",
      "- Total number of samples:  15927\n",
      "- Number of samples for training: 12742\n",
      "- Number of samples for validation:  3185\n",
      "-------------Done for PobleSec--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 23:44:02.879034: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-29 23:44:03.782705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14105 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:3b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "train_dbfile = open('train.pkl', 'rb')     \n",
    "train_dataset = pickle.load(train_dbfile)\n",
    "client_names = ['ElBorn','LesCorts','PobleSec']\n",
    "\n",
    "def dat(train_dataset,client_name):\n",
    "    # Load the data from the training dataset\n",
    "    processed_data = train_dataset.get(client_name).loc[:,['rnti_count', 'rb_down', 'rb_up', 'mcs_down', 'mcs_up', 'down', 'up']].astype('float32').fillna(0)\n",
    "\n",
    "    # Split data into training and validation\n",
    "    train_percentage = 0.8\n",
    "    num_samples_train = math.ceil(train_percentage*len(processed_data))\n",
    "    print(f\"- Total number of samples:  {len(processed_data)}\")\n",
    "    print(f\"- Number of samples for training: {num_samples_train}\")\n",
    "    print(f\"- Number of samples for validation:  {len(processed_data)-num_samples_train}\")\n",
    "    print(f\"-------------Done for {client_name}--------------------\")\n",
    "    train_data = processed_data[:num_samples_train]\n",
    "    val_data = processed_data[num_samples_train:]\n",
    "\n",
    "    scalerX = MinMaxScaler(feature_range=(-1,1))\n",
    "    scalerY = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "    # numpy arrays from here on\n",
    "    X_train = scalerX.fit_transform(train_data.loc[:,['rb_down', 'rb_up', 'rnti_count', 'mcs_down', 'mcs_up']])\n",
    "    X_train = X_train[..., np.newaxis] # CNN requested shape\n",
    "    X_val = scalerX.transform(val_data.loc[:,['rb_down', 'rb_up', 'rnti_count', 'mcs_down', 'mcs_up']])\n",
    "    X_val = X_val[..., np.newaxis] # CNN requested shape\n",
    "    Y_train = scalerY.fit_transform(train_data.loc[:,['rnti_count', 'rb_down', 'rb_up', 'down', 'up']])\n",
    "    Y_val = scalerY.transform(val_data.loc[:,['rnti_count', 'rb_down', 'rb_up', 'down', 'up']])\n",
    "\n",
    "    return X_train,Y_train,X_val,Y_val\n",
    "\n",
    "\n",
    "client_train_dataset = collections.OrderedDict()\n",
    "for i in range(3):\n",
    "    client_name = client_names[i]\n",
    "    x_train,y_train,x_val,y_val = dat(train_dataset,client_name)\n",
    "    data = collections.OrderedDict((('label', y_train), ('feat', x_train)))\n",
    "    client_train_dataset[client_name] = data\n",
    "\n",
    "train_dataset = tff.simulation.datasets.TestClientData(client_train_dataset)\n",
    "sample_dataset = train_dataset.create_tf_dataset_for_client(train_dataset.client_ids[0])\n",
    "sample_element = next(iter(sample_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROUNDS = 5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 20\n",
    "PREFETCH_BUFFER = 10\n",
    "SHUFFLE_BUFFER = 1000\n",
    "\n",
    "\n",
    "def preprocess(dataset):\n",
    "\n",
    "  def batch_format_fn(element):\n",
    "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
    "\n",
    "    return collections.OrderedDict(\n",
    "        x=element['feat'],\n",
    "        y=element['label'])\n",
    "\n",
    "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(\n",
    "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
    "\n",
    "preprocessed_sample_dataset = preprocess(sample_dataset)\n",
    "sample_batch = nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_sample_dataset)))\n",
    "\n",
    "\n",
    "def make_federated_data(client_data, client_ids):\n",
    "    return [preprocess(client_data.create_tf_dataset_for_client(x)) for x in client_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(inputShape, nVarsOut, nFuture=1, nFilters=16, kernelSize1=[16, 3], kernelSize2=[3, 5], kernelSize3=[8, 3],\n",
    "              kernelSize4=[4, 3], poolSize=[2, 1], mask='11111'):\n",
    "\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(filters=nFilters, kernel_size=kernelSize1, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[0])), name='Conv2D_1',input_shape=inputShape),\n",
    "        tf.keras.layers.Conv2D(filters=nFilters, kernel_size=kernelSize2, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[1])), name='Conv2D_2'),\n",
    "        tf.keras.layers.Conv2D(filters=nFilters * 2, kernel_size=kernelSize3, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[2])), name='Conv2D_3'),\n",
    "        tf.keras.layers.Conv2D(filters=nFilters * 2, kernel_size=kernelSize4, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[3])), name='Conv2D_4'),\n",
    "        tf.keras.layers.AveragePooling2D(pool_size=poolSize, name='AveragePooling'),\n",
    "        tf.keras.layers.Flatten(name='flatten'),\n",
    "        tf.keras.layers.Dense(nFuture * nVarsOut, activation='tanh', name='forecast', trainable=bool(int(mask[4])))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_example_dataset = preprocessed_sample_dataset\n",
    "federated_train_data = make_federated_data(train_dataset, train_dataset.client_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPast = 10                  # Number of past samples used for predictions (equivalent to 20 minutes)\n",
    "nFuture = 1                 # Number of future samples to be predicted (equivalent to 2 minutes)\n",
    "windowSize = 1              # Window size (time-series forecasting)\n",
    "nEpochs = 100                # Number of epochs\n",
    "batch_size = 128            # Batch size (other options: [64, 32])\n",
    "nRep = 2  \n",
    "nVarsOut = y_train.shape[1]             # Output shape\n",
    "inputShape = nPast, x_train.shape[1], 1 # Input shape based on the number of past observations and the shape of the validation data\n",
    "\n",
    "\n",
    "# genValLosses = TimeseriesGenerator(x_val, y_val, nPast, stride=windowSize, batch_size=1)  # BS always 1 here!\n",
    "# genTrain = TimeseriesGenerator(x_train, y_train, nPast, stride=windowSize, batch_size=batch_size)\n",
    "# genVal = TimeseriesGenerator(x_val, y_val, nPast, stride=windowSize, batch_size=batch_size)\n",
    "\n",
    "def model_fn():\n",
    "  keras_model = create_keras_model(inputShape, nVarsOut, nFuture)\n",
    "  return tff.learning.from_keras_model(keras_model, input_spec=preprocessed_example_dataset.element_spec, loss = [tf.keras.losses.SparseCategoricalCrossentropy()])\n",
    "\n",
    "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
    "    model_fn,\n",
    "    client_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate = 0.02),\n",
    "    server_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate = 1),\n",
    "    )\n",
    "state = iterative_process.initialize()\n",
    "output = iterative_process.next(state, fed_train_data)\n",
    "state = output.state\n",
    "metrics = output.metrics\n",
    "print('round 1, metrics={}'.format(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "genTrain = TimeseriesGenerator(x_train, y_train, nPast, stride=windowSize, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(genTrain.get_config()).shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tmp = pd.DataFrame.from_dict(genTrain)\n",
    "np.shape(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('fed': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31e2d1d5cfb83b98ad934fd33567af6c803d640f2ec0d15be4831a5eac94ddf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
