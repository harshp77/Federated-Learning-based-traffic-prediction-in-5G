{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow_federated as tff\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            down         up  rnti_count   mcs_down  \\\n",
      "2018-03-28 15:56:00  174876888.0  1856888.0       10229  15.332298   \n",
      "2018-03-28 15:58:00  209054184.0  2866200.0       12223  15.116846   \n",
      "2018-03-28 16:00:00  191464640.0  1935360.0       11152  15.215739   \n",
      "2018-03-28 16:02:00  241515688.0  2991152.0       14040  15.135400   \n",
      "2018-03-28 16:04:00  264131088.0  3288816.0       15247  15.188944   \n",
      "...                          ...        ...         ...        ...   \n",
      "2018-04-03 11:30:00  128110784.0  1088280.0        7758  14.980204   \n",
      "2018-04-03 11:32:00  151273208.0  1283992.0        9103  15.013859   \n",
      "2018-04-03 11:34:00  149891584.0  1222184.0        8923  15.202415   \n",
      "2018-04-03 11:36:00  155997488.0  1331536.0        9309  15.051294   \n",
      "2018-04-03 11:38:00  172544880.0  1846208.0       10338  15.219135   \n",
      "\n",
      "                     mcs_down_var     mcs_up  mcs_up_var   rb_down  \\\n",
      "2018-03-28 15:56:00     87.157688  14.981497   49.989484  0.029681   \n",
      "2018-03-28 15:58:00     87.192168  16.432612   62.494670  0.035971   \n",
      "2018-03-28 16:00:00     87.227955  15.885238   63.087007  0.032750   \n",
      "2018-03-28 16:02:00     86.199501  15.714660   77.187459  0.041372   \n",
      "2018-03-28 16:04:00     86.151119  15.414080   69.118561  0.045074   \n",
      "...                           ...        ...         ...       ...   \n",
      "2018-04-03 11:30:00     86.783498  14.489431   35.620732  0.021526   \n",
      "2018-04-03 11:32:00     87.903331  16.019702   42.272334  0.025785   \n",
      "2018-04-03 11:34:00     86.370911  16.476345   34.919566  0.025406   \n",
      "2018-04-03 11:36:00     87.701243  15.943630   51.888593  0.026372   \n",
      "2018-04-03 11:38:00     87.745979  16.592670   52.344327  0.029585   \n",
      "\n",
      "                      rb_down_var     rb_up     rb_up_var  \n",
      "2018-03-28 15:56:00  4.497698e-08  0.000541  3.143297e-08  \n",
      "2018-03-28 15:58:00  4.615535e-08  0.000852  4.439640e-08  \n",
      "2018-03-28 16:00:00  4.646104e-08  0.000607  2.993595e-08  \n",
      "2018-03-28 16:02:00  4.532153e-08  0.000925  5.382563e-08  \n",
      "2018-03-28 16:04:00  4.655543e-08  0.001021  5.922178e-08  \n",
      "...                           ...       ...           ...  \n",
      "2018-04-03 11:30:00  4.404124e-08  0.000380  1.945518e-08  \n",
      "2018-04-03 11:32:00  4.499877e-08  0.000444  2.165493e-08  \n",
      "2018-04-03 11:34:00  4.622009e-08  0.000373  1.820744e-08  \n",
      "2018-04-03 11:36:00  4.449000e-08  0.000388  1.931032e-08  \n",
      "2018-04-03 11:38:00  4.505553e-08  0.000563  3.314203e-08  \n",
      "\n",
      "[4192 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "train_dbfile = open('train.pkl', 'rb')     \n",
    "train_dataset = pickle.load(train_dbfile)\n",
    "test_dbfile = open('test.pkl', 'rb')     \n",
    "test_dataset = pickle.load(test_dbfile)\n",
    "\n",
    "Client_1 = train_dataset['ElBorn']\n",
    "Client_2 = train_dataset['LesCorts']\n",
    "Client_3 = train_dataset['PobleSec']\n",
    "\n",
    "print(Client_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'snippets': 'Yonder comes my master, your brother.'}, {'snippets': 'But do not so. I have five hundred crowns,'}, {'snippets': 'I scarce can speak to thank you for myself.\\n'}, {'snippets': \"[Coming forward] Sweet masters, be patient; for your father's\"}, {'snippets': \"remembrance, be at accord.\\nIs 'old dog' my reward? Most true, I have lost my teeth in\"}, {'snippets': 'Come not within these doors; within this roof\\nThe enemy of all your graces lives.\\nYour brother- no, no brother; yet the son-\\nYet not the son; I will not call him son\\nOf him I was about to call his father-\\nHath heard your praises; and this night he means\\nTo burn the lodging where you use to lie,\\nAnd you within it. If he fail of that,\\nHe will have other means to cut you off;\\nI overheard him and his practices.\\nThis is no place; this house is but a butchery;\\nAbhor it, fear it, do not enter it.\\nNo matter whither, so you come not here.'}, {'snippets': 'O my sweet master! O you memory\\nOf old Sir Rowland! Why, what make you here?\\nWhy are you virtuous? Why do people love you?\\nAnd wherefore are you gentle, strong, and valiant?\\nWhy would you be so fond to overcome\\nThe bonny prizer of the humorous Duke?\\nYour praise is come too swiftly home before you.\\nKnow you not, master, to some kind of men\\nTheir graces serve them but as enemies?\\nNo more do yours. Your virtues, gentle master,\\nAre sanctified and holy traitors to you.\\nO, what a world is this, when what is comely\\nEnvenoms him that bears it!\\nO unhappy youth!'}, {'snippets': \"The thrifty hire I sav'd under your father,\\nWhich I did store to be my foster-nurse,\\nWhen service should in my old limbs lie lame,\\nAnd unregarded age in corners thrown.\\nTake that, and He that doth the ravens feed,\\nYea, providently caters for the sparrow,\\nBe comfort to my age! Here is the gold;\\nAll this I give you. Let me be your servant;\\nThough I look old, yet I am strong and lusty;\\nFor in my youth I never did apply\\nHot and rebellious liquors in my blood,\\nNor did not with unbashful forehead woo\\nThe means of weakness and debility;\\nTherefore my age is as a lusty winter,\\nFrosty, but kindly. Let me go with you;\\nI'll do the service of a younger man\\nIn all your business and necessities.\\nMaster, go on; and I will follow the\"}, {'snippets': \"To the last gasp, with truth and loyalty.\\nFrom seventeen years till now almost four-score\\nHere lived I, but now live here no more.\\nAt seventeen years many their fortunes seek,\\nBut at fourscore it is too late a week;\\nYet fortune cannot recompense me better\\nThan to die well and not my master's debtor.          Exeunt\\nDear master, I can go no further. O, I die for food! Here lie\"}]\n"
     ]
    }
   ],
   "source": [
    "client_id_colname = 'character' # the column that represents client ID\n",
    "data_colname = 'snippets'\n",
    "def create_tff_dataset_for_csv_file(filepath):\n",
    "  df = pd.read_csv(filepath)\n",
    "  client_ids = df[client_id_colname].unique().tolist()\n",
    "  # print(client_ids)\n",
    "\n",
    "  def create_tf_dataset_for_client_fn(client_id):                       # helper Function that takes the client ids then filter out any null values\n",
    "    client_data = df[df[client_id_colname] == client_id]                    # Then selects only the data i.e the snippets\n",
    "    client_data = client_data[client_data[data_colname].notnull()]             # Then returns records dict with client id as key\n",
    "    client_data = client_data[[data_colname]]\n",
    "    records = client_data.to_dict('records')\n",
    "    # print(client_data)\n",
    "    print(records)\n",
    "\n",
    "    def dataset_gen():\n",
    "      for row in records:\n",
    "        yield row\n",
    "    return tf.data.Dataset.from_generator(dataset_gen,\n",
    "                                          output_types={data_colname: tf.string},\n",
    "                                          output_shapes={data_colname: []}\n",
    "    )\n",
    "\n",
    "  return tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "      client_ids=client_ids,\n",
    "      serializable_dataset_fn=create_tf_dataset_for_client_fn\n",
    "  )\n",
    "\n",
    "filepath = 'shakespeare_train.csv'\n",
    "train_data = create_tff_dataset_for_csv_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tff\u001b[38;5;241m.\u001b[39msimulation\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mClientData\u001b[38;5;241m.\u001b[39mfrom_clients_and_tf_fn(\n\u001b[1;32m     20\u001b[0m       client_ids\u001b[38;5;241m=\u001b[39mclient_ids,\n\u001b[1;32m     21\u001b[0m       serializable_dataset_fn\u001b[38;5;241m=\u001b[39mcreate_tf_dataset_for_client_fn\n\u001b[1;32m     22\u001b[0m   )\n\u001b[1;32m     24\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 25\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_tff_dataset_for_csv_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [6], line 19\u001b[0m, in \u001b[0;36mcreate_tff_dataset_for_csv_file\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[38;5;28;01myield\u001b[39;00m row\n\u001b[1;32m     14\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_generator(dataset_gen,\n\u001b[1;32m     15\u001b[0m                                         output_types\u001b[38;5;241m=\u001b[39m{data_colname: tf\u001b[38;5;241m.\u001b[39mstring},\n\u001b[1;32m     16\u001b[0m                                         output_shapes\u001b[38;5;241m=\u001b[39m{data_colname: []}\n\u001b[1;32m     17\u001b[0m   )\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClientData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_clients_and_tf_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserializable_dataset_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_tf_dataset_for_client_fn\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/simulation/datasets/client_data.py:273\u001b[0m, in \u001b[0;36mClientData.from_clients_and_tf_fn\u001b[0;34m(cls, client_ids, serializable_dataset_fn)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_clients_and_tf_fn\u001b[39m(\n\u001b[1;32m    256\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m    257\u001b[0m     client_ids: Iterable[\u001b[39mstr\u001b[39m],\n\u001b[1;32m    258\u001b[0m     serializable_dataset_fn: Callable[[\u001b[39mstr\u001b[39m], tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset],\n\u001b[1;32m    259\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m'\u001b[39m\u001b[39mClientData\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    260\u001b[0m   \u001b[39m\"\"\"Constructs a `ClientData` based on the given function.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \n\u001b[1;32m    262\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m    A `ClientData` object.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m   \u001b[39mreturn\u001b[39;00m ConcreteClientData(client_ids, serializable_dataset_fn)\n",
      "File \u001b[0;32m~/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/simulation/datasets/client_data.py:433\u001b[0m, in \u001b[0;36mConcreteClientData.__init__\u001b[0;34m(self, client_ids, serializable_dataset_fn)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client_ids \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(client_ids)\n\u001b[1;32m    431\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_serializable_dataset_fn \u001b[39m=\u001b[39m serializable_dataset_fn\n\u001b[0;32m--> 433\u001b[0m example_dataset \u001b[39m=\u001b[39m serializable_dataset_fn(\u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(client_ids)))\n\u001b[1;32m    434\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_type_structure \u001b[39m=\u001b[39m example_dataset\u001b[39m.\u001b[39melement_spec\n",
      "Cell \u001b[0;32mIn [6], line 15\u001b[0m, in \u001b[0;36mcreate_tff_dataset_for_csv_file.<locals>.create_tf_dataset_for_client_fn\u001b[0;34m(client_id)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m row\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_generator(dataset_gen,\n\u001b[0;32m---> 15\u001b[0m                                       output_types\u001b[38;5;241m=\u001b[39m{data_colname: tf\u001b[38;5;241m.\u001b[39mstring},\n\u001b[1;32m     16\u001b[0m                                       output_shapes\u001b[38;5;241m=\u001b[39m{data_colname: []}\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "data_colname = [\"down\",\"up\",\"rnti_count\"]\n",
    "def create_tff_dataset_for_csv_file(filepath):\n",
    "  df = pd.read_pickle('train.pkl')   \n",
    "  client_ids = ['ElBorn','LesCorts','PobleSec']\n",
    "\n",
    "  def create_tf_dataset_for_client_fn(client_id):                       # helper Function that takes the client ids then filter out any null values\n",
    "    client_data = df[client_id]                    # Then selects only the data i.e the snippets\n",
    "    client_data = client_data[data_colname]\n",
    "    records = client_data.to_dict('records')\n",
    "\n",
    "    def dataset_gen():\n",
    "      for row in records:\n",
    "        yield row\n",
    "    return tf.data.Dataset.from_generator(dataset_gen,\n",
    "                                          output_types={data_colname: tf.string},\n",
    "                                          output_shapes={data_colname: []}\n",
    "    )\n",
    "\n",
    "  return tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "      client_ids=client_ids,\n",
    "      serializable_dataset_fn=create_tf_dataset_for_client_fn\n",
    "  )\n",
    "\n",
    "filepath = 'train.pkl'\n",
    "train_data = create_tff_dataset_for_csv_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>down</th>\n",
       "      <th>up</th>\n",
       "      <th>rnti_count</th>\n",
       "      <th>mcs_down</th>\n",
       "      <th>mcs_down_var</th>\n",
       "      <th>mcs_up</th>\n",
       "      <th>mcs_up_var</th>\n",
       "      <th>rb_down</th>\n",
       "      <th>rb_down_var</th>\n",
       "      <th>rb_up</th>\n",
       "      <th>rb_up_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:12:00</th>\n",
       "      <td>38490328.0</td>\n",
       "      <td>18729576.0</td>\n",
       "      <td>4495</td>\n",
       "      <td>2.573617</td>\n",
       "      <td>45.616769</td>\n",
       "      <td>13.382755</td>\n",
       "      <td>71.070384</td>\n",
       "      <td>0.013610</td>\n",
       "      <td>1.961025e-08</td>\n",
       "      <td>0.008343</td>\n",
       "      <td>1.083682e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:14:00</th>\n",
       "      <td>28093304.0</td>\n",
       "      <td>19340160.0</td>\n",
       "      <td>4277</td>\n",
       "      <td>2.104966</td>\n",
       "      <td>38.916538</td>\n",
       "      <td>14.779940</td>\n",
       "      <td>75.957494</td>\n",
       "      <td>0.012278</td>\n",
       "      <td>1.644211e-08</td>\n",
       "      <td>0.007040</td>\n",
       "      <td>1.061491e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:16:00</th>\n",
       "      <td>32971216.0</td>\n",
       "      <td>20630232.0</td>\n",
       "      <td>4729</td>\n",
       "      <td>2.452463</td>\n",
       "      <td>45.055805</td>\n",
       "      <td>14.624951</td>\n",
       "      <td>72.153009</td>\n",
       "      <td>0.013160</td>\n",
       "      <td>1.761309e-08</td>\n",
       "      <td>0.009001</td>\n",
       "      <td>9.109703e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:18:00</th>\n",
       "      <td>28745776.0</td>\n",
       "      <td>13323504.0</td>\n",
       "      <td>3924</td>\n",
       "      <td>2.072077</td>\n",
       "      <td>37.965933</td>\n",
       "      <td>13.979430</td>\n",
       "      <td>71.980549</td>\n",
       "      <td>0.012282</td>\n",
       "      <td>1.668480e-08</td>\n",
       "      <td>0.006028</td>\n",
       "      <td>5.813802e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:20:00</th>\n",
       "      <td>30887688.0</td>\n",
       "      <td>13812144.0</td>\n",
       "      <td>4099</td>\n",
       "      <td>2.241337</td>\n",
       "      <td>40.990956</td>\n",
       "      <td>13.314127</td>\n",
       "      <td>77.085867</td>\n",
       "      <td>0.012741</td>\n",
       "      <td>1.721707e-08</td>\n",
       "      <td>0.006706</td>\n",
       "      <td>7.246522e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22 06:46:00</th>\n",
       "      <td>48370808.0</td>\n",
       "      <td>231664.0</td>\n",
       "      <td>2929</td>\n",
       "      <td>15.198009</td>\n",
       "      <td>85.244211</td>\n",
       "      <td>14.277778</td>\n",
       "      <td>25.174603</td>\n",
       "      <td>0.008044</td>\n",
       "      <td>4.418387e-08</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>3.346528e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22 06:48:00</th>\n",
       "      <td>44972096.0</td>\n",
       "      <td>184784.0</td>\n",
       "      <td>2847</td>\n",
       "      <td>15.449166</td>\n",
       "      <td>87.724101</td>\n",
       "      <td>14.988889</td>\n",
       "      <td>24.066667</td>\n",
       "      <td>0.007614</td>\n",
       "      <td>4.375022e-08</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>3.466806e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22 06:50:00</th>\n",
       "      <td>31460120.0</td>\n",
       "      <td>33088.0</td>\n",
       "      <td>1997</td>\n",
       "      <td>15.330073</td>\n",
       "      <td>86.727797</td>\n",
       "      <td>12.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>4.269245e-08</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22 06:52:00</th>\n",
       "      <td>28048016.0</td>\n",
       "      <td>48072.0</td>\n",
       "      <td>1748</td>\n",
       "      <td>15.351619</td>\n",
       "      <td>92.460379</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.004745</td>\n",
       "      <td>4.644352e-08</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>2.041667e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22 06:54:00</th>\n",
       "      <td>41325736.0</td>\n",
       "      <td>158248.0</td>\n",
       "      <td>2563</td>\n",
       "      <td>15.173839</td>\n",
       "      <td>89.072241</td>\n",
       "      <td>16.550000</td>\n",
       "      <td>36.650000</td>\n",
       "      <td>0.006851</td>\n",
       "      <td>4.615977e-08</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>4.252500e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6892 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           down          up  rnti_count   mcs_down  \\\n",
       "2019-01-12 17:12:00  38490328.0  18729576.0        4495   2.573617   \n",
       "2019-01-12 17:14:00  28093304.0  19340160.0        4277   2.104966   \n",
       "2019-01-12 17:16:00  32971216.0  20630232.0        4729   2.452463   \n",
       "2019-01-12 17:18:00  28745776.0  13323504.0        3924   2.072077   \n",
       "2019-01-12 17:20:00  30887688.0  13812144.0        4099   2.241337   \n",
       "...                         ...         ...         ...        ...   \n",
       "2019-01-22 06:46:00  48370808.0    231664.0        2929  15.198009   \n",
       "2019-01-22 06:48:00  44972096.0    184784.0        2847  15.449166   \n",
       "2019-01-22 06:50:00  31460120.0     33088.0        1997  15.330073   \n",
       "2019-01-22 06:52:00  28048016.0     48072.0        1748  15.351619   \n",
       "2019-01-22 06:54:00  41325736.0    158248.0        2563  15.173839   \n",
       "\n",
       "                     mcs_down_var     mcs_up  mcs_up_var   rb_down  \\\n",
       "2019-01-12 17:12:00     45.616769  13.382755   71.070384  0.013610   \n",
       "2019-01-12 17:14:00     38.916538  14.779940   75.957494  0.012278   \n",
       "2019-01-12 17:16:00     45.055805  14.624951   72.153009  0.013160   \n",
       "2019-01-12 17:18:00     37.965933  13.979430   71.980549  0.012282   \n",
       "2019-01-12 17:20:00     40.990956  13.314127   77.085867  0.012741   \n",
       "...                           ...        ...         ...       ...   \n",
       "2019-01-22 06:46:00     85.244211  14.277778   25.174603  0.008044   \n",
       "2019-01-22 06:48:00     87.724101  14.988889   24.066667  0.007614   \n",
       "2019-01-22 06:50:00     86.727797  12.111111    0.000000  0.005174   \n",
       "2019-01-22 06:52:00     92.460379   7.900000    1.800000  0.004745   \n",
       "2019-01-22 06:54:00     89.072241  16.550000   36.650000  0.006851   \n",
       "\n",
       "                      rb_down_var     rb_up     rb_up_var  \n",
       "2019-01-12 17:12:00  1.961025e-08  0.008343  1.083682e-08  \n",
       "2019-01-12 17:14:00  1.644211e-08  0.007040  1.061491e-08  \n",
       "2019-01-12 17:16:00  1.761309e-08  0.009001  9.109703e-09  \n",
       "2019-01-12 17:18:00  1.668480e-08  0.006028  5.813802e-09  \n",
       "2019-01-12 17:20:00  1.721707e-08  0.006706  7.246522e-09  \n",
       "...                           ...       ...           ...  \n",
       "2019-01-22 06:46:00  4.418387e-08  0.000096  3.346528e-09  \n",
       "2019-01-22 06:48:00  4.375022e-08  0.000056  3.466806e-09  \n",
       "2019-01-22 06:50:00  4.269245e-08  0.000014  0.000000e+00  \n",
       "2019-01-22 06:52:00  4.644352e-08  0.000025  2.041667e-09  \n",
       "2019-01-22 06:54:00  4.615977e-08  0.000069  4.252500e-09  \n",
       "\n",
       "[6892 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"./dummy.pkl\")\n",
    "# [\"down\",\"up\",\"rnti_count\",\"mcs_down\",\"mcs_down_var\",\"mcs_up\",\"mcs_up_var\",\"rb_down\",\"rb_down_var\",\"rb_up\",\"rb_up_var\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow import reshape, nest, config\n",
    "import tensorflow_federated as tff\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.layers import Input, GRU, Dense, Flatten, Reshape, BatchNormalization, LayerNormalization, Conv2D, \\\n",
    "    SeparableConv2D, AveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError as MSE, MeanAbsoluteError as MAE\n",
    "from keras.metrics import MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dbfile = open('train.pkl', 'rb')     \n",
    "train_dataset = pickle.load(train_dbfile)\n",
    "test_dbfile = open('test.pkl', 'rb')     \n",
    "test_dataset = pickle.load(test_dbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Total number of samples:  4192\n",
      "- Number of samples for training: 3354\n",
      "- Number of samples for validation:  838\n",
      "-------------Done for ElBorn--------------------\n",
      "- Total number of samples:  6892\n",
      "- Number of samples for training: 5514\n",
      "- Number of samples for validation:  1378\n",
      "-------------Done for LesCorts--------------------\n",
      "- Total number of samples:  15927\n",
      "- Number of samples for training: 12742\n",
      "- Number of samples for validation:  3185\n",
      "-------------Done for PobleSec--------------------\n"
     ]
    }
   ],
   "source": [
    "train_dbfile = open('train.pkl', 'rb')     \n",
    "train_dataset = pickle.load(train_dbfile)\n",
    "client_names = ['ElBorn','LesCorts','PobleSec']\n",
    "\n",
    "def dat(train_dataset,client_name):\n",
    "    # Load the data from the training dataset\n",
    "    processed_data = train_dataset.get(client_name).loc[:,['rnti_count', 'rb_down', 'rb_up', 'mcs_down', 'mcs_up', 'down', 'up']].astype('float32').fillna(0)\n",
    "\n",
    "    # Split data into training and validation\n",
    "    train_percentage = 0.8\n",
    "    num_samples_train = math.ceil(train_percentage*len(processed_data))\n",
    "    print(f\"- Total number of samples:  {len(processed_data)}\")\n",
    "    print(f\"- Number of samples for training: {num_samples_train}\")\n",
    "    print(f\"- Number of samples for validation:  {len(processed_data)-num_samples_train}\")\n",
    "    print(f\"-------------Done for {client_name}--------------------\")\n",
    "    train_data = processed_data[:num_samples_train]\n",
    "    val_data = processed_data[num_samples_train:]\n",
    "\n",
    "    scalerX = MinMaxScaler(feature_range=(-1,1))\n",
    "    scalerY = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "    # numpy arrays from here on\n",
    "    X_train = scalerX.fit_transform(train_data.loc[:,['rb_down', 'rb_up', 'rnti_count', 'mcs_down', 'mcs_up']])\n",
    "    X_train = X_train[..., np.newaxis] # CNN requested shape\n",
    "    X_val = scalerX.transform(val_data.loc[:,['rb_down', 'rb_up', 'rnti_count', 'mcs_down', 'mcs_up']])\n",
    "    X_val = X_val[..., np.newaxis] # CNN requested shape\n",
    "    Y_train = scalerY.fit_transform(train_data.loc[:,['rnti_count', 'rb_down', 'rb_up', 'down', 'up']])\n",
    "    Y_val = scalerY.transform(val_data.loc[:,['rnti_count', 'rb_down', 'rb_up', 'down', 'up']])\n",
    "\n",
    "    return X_train,Y_train,X_val,Y_val\n",
    "\n",
    "\n",
    "client_train_dataset = collections.OrderedDict()\n",
    "for i in range(3):\n",
    "    client_name = client_names[i]\n",
    "    x_train,y_train,x_val,y_val = dat(train_dataset,client_name)\n",
    "    data = collections.OrderedDict((('label', y_train), ('feat', x_train)))\n",
    "    client_train_dataset[client_name] = data\n",
    "\n",
    "train_dataset = tff.simulation.datasets.TestClientData(client_train_dataset)\n",
    "sample_dataset = train_dataset.create_tf_dataset_for_client(train_dataset.client_ids[0])\n",
    "sample_element = next(iter(sample_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROUNDS = 5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 20\n",
    "PREFETCH_BUFFER = 10\n",
    "SHUFFLE_BUFFER = 1000\n",
    "\n",
    "def preprocess(dataset):\n",
    "\n",
    "  def batch_format_fn(element):\n",
    "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
    "\n",
    "    return collections.OrderedDict(\n",
    "        x=element['feat'],\n",
    "        y=element['label'])\n",
    "\n",
    "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(\n",
    "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
    "\n",
    "preprocessed_sample_dataset = preprocess(sample_dataset)\n",
    "sample_batch = nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_sample_dataset)))\n",
    "\n",
    "\n",
    "def make_federated_data(client_data, client_ids):\n",
    "    return [preprocess(client_data.create_tf_dataset_for_client(x)) for x in client_ids]\n",
    "\n",
    "federated_train_data = make_federated_data(train_dataset, train_dataset.client_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of client datasets: 3\n",
      "First dataset: <PrefetchDataset element_spec=OrderedDict([('x', TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name=None)), ('y', TensorSpec(shape=(None, 5), dtype=tf.float32, name=None))])>\n"
     ]
    }
   ],
   "source": [
    "print('Number of client datasets: {l}'.format(l=len(federated_train_data)))\n",
    "print('First dataset: {d}'.format(d=federated_train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(inputShape, nVarsOut, nFuture=1, nFilters=16, kernelSize1=[16, 3], kernelSize2=[3, 5], kernelSize3=[8, 3],\n",
    "              kernelSize4=[4, 3], poolSize=[2, 1], mask='11111'):\n",
    "  \n",
    "    model = Sequential([\n",
    "        Conv2D(filters=nFilters, kernel_size=kernelSize1, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[0])), name='Conv2D_1',\n",
    "               input_shape=inputShape),\n",
    "        Conv2D(filters=nFilters, kernel_size=kernelSize2, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[1])), name='Conv2D_2'),\n",
    "        Conv2D(filters=nFilters * 2, kernel_size=kernelSize3, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[2])), name='Conv2D_3'),\n",
    "        Conv2D(filters=nFilters * 2, kernel_size=kernelSize4, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[3])), name='Conv2D_4'),\n",
    "        AveragePooling2D(pool_size=poolSize, name='AveragePooling'),\n",
    "        Flatten(name='flatten'),\n",
    "        Dense(nFuture * nVarsOut, activation='tanh', name='forecast', trainable=bool(int(mask[4])))  # [-1, 1]\n",
    "    ],\n",
    "        name=f'CNN_{nPast}to{nFuture}_{nFilters}-{kernelSize1[0]}x{kernelSize1[1]}_{nFilters}-{kernelSize2[0]}x{kernelSize2[1]}_{nFilters * 2}-{kernelSize3[0]}x{kernelSize3[1]}_{nFilters * 2}-{kernelSize4[0]}x{kernelSize4[1]}_finalPooling{poolSize[0]}x{poolSize[1]}_' + mask\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss=MSE(),\n",
    "                  metrics=[MeanAbsoluteError()])\n",
    "\n",
    "    print(model.summary(), flush=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_10to1_16-16x3_16-3x5_32-8x3_32-4x3_finalPooling2x1_11111\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv2D_1 (Conv2D)           (None, 10, 5, 16)         784       \n",
      "                                                                 \n",
      " Conv2D_2 (Conv2D)           (None, 10, 5, 16)         3856      \n",
      "                                                                 \n",
      " Conv2D_3 (Conv2D)           (None, 10, 5, 32)         12320     \n",
      "                                                                 \n",
      " Conv2D_4 (Conv2D)           (None, 10, 5, 32)         12320     \n",
      "                                                                 \n",
      " AveragePooling (AveragePool  (None, 5, 5, 32)         0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 800)               0         \n",
      "                                                                 \n",
      " forecast (Dense)            (None, 5)                 4005      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,285\n",
      "Trainable params: 33,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 11:40:31.266584: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:421] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2022-10-29 11:40:31.268385: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_ops_fused_impl.h:621 : UNIMPLEMENTED: DNN library is not found.\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'CNN_10to1_16-16x3_16-3x5_32-8x3_32-4x3_finalPooling2x1_11111/Conv2D_1/Relu' defined at (most recent call last):\n    File \"/home/isbt/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/isbt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/isbt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/isbt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/isbt/anaconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_cell\n      result = self._run_cell(\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3139, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3318, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_22805/1486744402.py\", line 26, in <module>\n      history = model.fit(genTrain, epochs=nEpochs, validation_data=genVal, verbose=2)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/training.py\", line 1673, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/training.py\", line 1272, in train_function\n      return step_function(self, iterator)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/training.py\", line 1256, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/training.py\", line 1245, in run_step\n      outputs = model.train_step(data)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/training.py\", line 1046, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/sequential.py\", line 413, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/layers/convolutional/base_conv.py\", line 314, in call\n      return self.activation(outputs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/activations.py\", line 317, in relu\n      return backend.relu(\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/backend.py\", line 5369, in relu\n      x = tf.nn.relu(x)\nNode: 'CNN_10to1_16-16x3_16-3x5_32-8x3_32-4x3_finalPooling2x1_11111/Conv2D_1/Relu'\nDNN library is not found.\n\t [[{{node CNN_10to1_16-16x3_16-3x5_32-8x3_32-4x3_finalPooling2x1_11111/Conv2D_1/Relu}}]] [Op:__inference_train_function_4393]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Train the model for nRep repetitions\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nRep):                \n\u001b[0;32m---> 26\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnEpochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenVal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     avg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     28\u001b[0m     avg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'CNN_10to1_16-16x3_16-3x5_32-8x3_32-4x3_finalPooling2x1_11111/Conv2D_1/Relu' defined at (most recent call last):\n    File \"/home/isbt/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/isbt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/isbt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/isbt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/isbt/anaconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_cell\n      result = self._run_cell(\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3139, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3318, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_22805/1486744402.py\", line 26, in <module>\n      history = model.fit(genTrain, epochs=nEpochs, validation_data=genVal, verbose=2)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/training.py\", line 1673, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/training.py\", line 1272, in train_function\n      return step_function(self, iterator)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/training.py\", line 1256, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/training.py\", line 1245, in run_step\n      outputs = model.train_step(data)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/training.py\", line 1046, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/sequential.py\", line 413, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/layers/convolutional/base_conv.py\", line 314, in call\n      return self.activation(outputs)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/activations.py\", line 317, in relu\n      return backend.relu(\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/backend.py\", line 5369, in relu\n      x = tf.nn.relu(x)\nNode: 'CNN_10to1_16-16x3_16-3x5_32-8x3_32-4x3_finalPooling2x1_11111/Conv2D_1/Relu'\nDNN library is not found.\n\t [[{{node CNN_10to1_16-16x3_16-3x5_32-8x3_32-4x3_finalPooling2x1_11111/Conv2D_1/Relu}}]] [Op:__inference_train_function_4393]"
     ]
    }
   ],
   "source": [
    "nPast = 10                  # Number of past samples used for predictions (equivalent to 20 minutes)\n",
    "nFuture = 1                 # Number of future samples to be predicted (equivalent to 2 minutes)\n",
    "windowSize = 1              # Window size (time-series forecasting)\n",
    "nEpochs = 10                # Number of epochs\n",
    "batch_size = 128            # Batch size (other options: [64, 32])\n",
    "nRep = 1                    # Number of repetitions whereby the model will be trained\n",
    "\n",
    "nVarsOut = y_val.shape[1]             # Output shape\n",
    "inputShape = nPast, x_val.shape[1], 1 # Input shape based on the number of past observations and the shape of the validation data\n",
    "\n",
    "loss1 = MSE(reduction='sum')\n",
    "loss2 = MAE(reduction='sum')\n",
    "\n",
    "model = CNN(inputShape, nVarsOut, nFuture)\n",
    "# Define the train and validation time-series\n",
    "genValLosses = TimeseriesGenerator(x_val, y_val, nPast, stride=windowSize, batch_size=1)  # BS always 1 here!\n",
    "genTrain = TimeseriesGenerator(x_train, y_train, nPast, stride=windowSize, batch_size=batch_size)\n",
    "genVal = TimeseriesGenerator(x_val, y_val, nPast, stride=windowSize, batch_size=batch_size)\n",
    "\n",
    "# Define arrays to store performance metrics \n",
    "avg = {'loss': np.zeros(nEpochs), 'val_loss': np.zeros(nEpochs), 'mean_absolute_error': np.zeros(nEpochs),\n",
    "        'val_mean_absolute_error': np.zeros(nEpochs)}\n",
    "\n",
    "# Train the model for nRep repetitions\n",
    "for i in range(nRep):                \n",
    "    history = model.fit(genTrain, epochs=nEpochs, validation_data=genVal, verbose=2)\n",
    "    avg['loss'] += np.array(history.history['loss'])\n",
    "    avg['val_loss'] += np.array(history.history['val_loss'])\n",
    "    avg['mean_absolute_error'] += np.array(history.history['mean_absolute_error'])\n",
    "    avg['val_mean_absolute_error'] += np.array(history.history['val_mean_absolute_error'])\n",
    "\n",
    "# Compute the average performance obtained across nRep repetitions\n",
    "avg = {k: v / nRep for k, v in avg.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 11:33:11.248590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1614] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13244 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:3b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fed Dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 23:43:57.872539: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-29 23:43:58.016003: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-29 23:43:58.055213: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-29 23:43:58.631565: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-29 23:43:58.631642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-29 23:43:58.631651: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow import reshape, nest, config\n",
    "import tensorflow_federated as tff\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.layers import Input, GRU, Dense, Flatten, Reshape, BatchNormalization, LayerNormalization, Conv2D, \\\n",
    "    SeparableConv2D, AveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError as MSE, MeanAbsoluteError as MAE\n",
    "from keras.metrics import MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Total number of samples:  4192\n",
      "- Number of samples for training: 3354\n",
      "- Number of samples for validation:  838\n",
      "-------------Done for ElBorn--------------------\n",
      "- Total number of samples:  6892\n",
      "- Number of samples for training: 5514\n",
      "- Number of samples for validation:  1378\n",
      "-------------Done for LesCorts--------------------\n",
      "- Total number of samples:  15927\n",
      "- Number of samples for training: 12742\n",
      "- Number of samples for validation:  3185\n",
      "-------------Done for PobleSec--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 23:44:02.879034: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-29 23:44:03.782705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14105 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:3b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "train_dbfile = open('train.pkl', 'rb')     \n",
    "train_dataset = pickle.load(train_dbfile)\n",
    "client_names = ['ElBorn','LesCorts','PobleSec']\n",
    "\n",
    "def dat(train_dataset,client_name):\n",
    "    # Load the data from the training dataset\n",
    "    processed_data = train_dataset.get(client_name).loc[:,['rnti_count', 'rb_down', 'rb_up', 'mcs_down', 'mcs_up', 'down', 'up']].astype('float32').fillna(0)\n",
    "\n",
    "    # Split data into training and validation\n",
    "    train_percentage = 0.8\n",
    "    num_samples_train = math.ceil(train_percentage*len(processed_data))\n",
    "    print(f\"- Total number of samples:  {len(processed_data)}\")\n",
    "    print(f\"- Number of samples for training: {num_samples_train}\")\n",
    "    print(f\"- Number of samples for validation:  {len(processed_data)-num_samples_train}\")\n",
    "    print(f\"-------------Done for {client_name}--------------------\")\n",
    "    train_data = processed_data[:num_samples_train]\n",
    "    val_data = processed_data[num_samples_train:]\n",
    "\n",
    "    scalerX = MinMaxScaler(feature_range=(-1,1))\n",
    "    scalerY = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "    # numpy arrays from here on\n",
    "    X_train = scalerX.fit_transform(train_data.loc[:,['rb_down', 'rb_up', 'rnti_count', 'mcs_down', 'mcs_up']])\n",
    "    X_train = X_train[..., np.newaxis] # CNN requested shape\n",
    "    X_val = scalerX.transform(val_data.loc[:,['rb_down', 'rb_up', 'rnti_count', 'mcs_down', 'mcs_up']])\n",
    "    X_val = X_val[..., np.newaxis] # CNN requested shape\n",
    "    Y_train = scalerY.fit_transform(train_data.loc[:,['rnti_count', 'rb_down', 'rb_up', 'down', 'up']])\n",
    "    Y_val = scalerY.transform(val_data.loc[:,['rnti_count', 'rb_down', 'rb_up', 'down', 'up']])\n",
    "\n",
    "    return X_train,Y_train,X_val,Y_val\n",
    "\n",
    "\n",
    "client_train_dataset = collections.OrderedDict()\n",
    "for i in range(3):\n",
    "    client_name = client_names[i]\n",
    "    x_train,y_train,x_val,y_val = dat(train_dataset,client_name)\n",
    "    data = collections.OrderedDict((('label', y_train), ('feat', x_train)))\n",
    "    client_train_dataset[client_name] = data\n",
    "\n",
    "train_dataset = tff.simulation.datasets.TestClientData(client_train_dataset)\n",
    "sample_dataset = train_dataset.create_tf_dataset_for_client(train_dataset.client_ids[0])\n",
    "sample_element = next(iter(sample_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROUNDS = 5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 20\n",
    "PREFETCH_BUFFER = 10\n",
    "SHUFFLE_BUFFER = 1000\n",
    "\n",
    "\n",
    "def preprocess(dataset):\n",
    "\n",
    "  def batch_format_fn(element):\n",
    "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
    "\n",
    "    return collections.OrderedDict(\n",
    "        x=element['feat'],\n",
    "        y=element['label'])\n",
    "\n",
    "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(\n",
    "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
    "\n",
    "preprocessed_sample_dataset = preprocess(sample_dataset)\n",
    "sample_batch = nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_sample_dataset)))\n",
    "\n",
    "\n",
    "def make_federated_data(client_data, client_ids):\n",
    "    return [preprocess(client_data.create_tf_dataset_for_client(x)) for x in client_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(inputShape, nVarsOut, nFuture=1, nFilters=16, kernelSize1=[16, 3], kernelSize2=[3, 5], kernelSize3=[8, 3],\n",
    "              kernelSize4=[4, 3], poolSize=[2, 1], mask='11111'):\n",
    "\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(filters=nFilters, kernel_size=kernelSize1, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[0])), name='Conv2D_1',input_shape=inputShape),\n",
    "        tf.keras.layers.Conv2D(filters=nFilters, kernel_size=kernelSize2, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[1])), name='Conv2D_2'),\n",
    "        tf.keras.layers.Conv2D(filters=nFilters * 2, kernel_size=kernelSize3, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[2])), name='Conv2D_3'),\n",
    "        tf.keras.layers.Conv2D(filters=nFilters * 2, kernel_size=kernelSize4, padding=\"same\", activation=\"relu\",\n",
    "               trainable=bool(int(mask[3])), name='Conv2D_4'),\n",
    "        tf.keras.layers.AveragePooling2D(pool_size=poolSize, name='AveragePooling'),\n",
    "        tf.keras.layers.Flatten(name='flatten'),\n",
    "        tf.keras.layers.Dense(nFuture * nVarsOut, activation='tanh', name='forecast', trainable=bool(int(mask[4])))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_example_dataset = preprocessed_sample_dataset\n",
    "federated_train_data = make_federated_data(train_dataset, train_dataset.client_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py\", line 179, in reduce_fn  *\n        output = model.forward_pass(batch, training=True)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/learning/framework/dataset_reduce.py\", line 33, in _dataset_reduce_fn  *\n        return dataset.reduce(initial_state=initial_state_fn(), reduce_func=reduce_fn)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 455, in forward_pass  *\n        return self._forward_pass(batch_input, training=training)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 408, in _forward_pass  *\n        predictions = self.predict_on_batch(inputs, training)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 398, in predict_on_batch  *\n        return self._keras_model(x, training=training)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 10, 5, 1), found shape=(None, 5, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m   keras_model \u001b[38;5;241m=\u001b[39m create_keras_model(inputShape, nVarsOut, nFuture)\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tff\u001b[38;5;241m.\u001b[39mlearning\u001b[38;5;241m.\u001b[39mfrom_keras_model(keras_model, input_spec\u001b[38;5;241m=\u001b[39mpreprocessed_example_dataset\u001b[38;5;241m.\u001b[39melement_spec, loss \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy()])\n\u001b[0;32m---> 14\u001b[0m iterative_process \u001b[38;5;241m=\u001b[39m \u001b[43mtff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_weighted_fed_avg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_optimizer_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_optimizer_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m state \u001b[38;5;241m=\u001b[39m iterative_process\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[1;32m     20\u001b[0m output \u001b[38;5;241m=\u001b[39m iterative_process\u001b[38;5;241m.\u001b[39mnext(state, fed_train_data)\n",
      "File \u001b[0;32m~/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/learning/algorithms/fed_avg.py:177\u001b[0m, in \u001b[0;36mbuild_weighted_fed_avg\u001b[0;34m(model_fn, client_optimizer_fn, server_optimizer_fn, client_weighting, model_distributor, model_aggregator, metrics_aggregator, use_experimental_simulation_loop)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m metrics_aggregator \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m   metrics_aggregator \u001b[39m=\u001b[39m metric_aggregator\u001b[39m.\u001b[39msum_then_finalize\n\u001b[0;32m--> 177\u001b[0m client_work \u001b[39m=\u001b[39m model_delta_client_work\u001b[39m.\u001b[39;49mbuild_model_delta_client_work(\n\u001b[1;32m    178\u001b[0m     model_fn\u001b[39m=\u001b[39;49mmodel_fn,\n\u001b[1;32m    179\u001b[0m     optimizer\u001b[39m=\u001b[39;49mclient_optimizer_fn,\n\u001b[1;32m    180\u001b[0m     client_weighting\u001b[39m=\u001b[39;49mclient_weighting,\n\u001b[1;32m    181\u001b[0m     metrics_aggregator\u001b[39m=\u001b[39;49mmetrics_aggregator,\n\u001b[1;32m    182\u001b[0m     use_experimental_simulation_loop\u001b[39m=\u001b[39;49muse_experimental_simulation_loop)\n\u001b[1;32m    183\u001b[0m finalizer \u001b[39m=\u001b[39m finalizers\u001b[39m.\u001b[39mbuild_apply_optimizer_finalizer(\n\u001b[1;32m    184\u001b[0m     server_optimizer_fn, model_weights_type)\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m composers\u001b[39m.\u001b[39mcompose_learning_process(initial_model_weights_fn,\n\u001b[1;32m    186\u001b[0m                                           model_distributor, client_work,\n\u001b[1;32m    187\u001b[0m                                           aggregator, finalizer)\n",
      "File \u001b[0;32m~/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py:324\u001b[0m, in \u001b[0;36mbuild_model_delta_client_work\u001b[0;34m(model_fn, optimizer, client_weighting, delta_l2_regularizer, metrics_aggregator, use_experimental_simulation_loop)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[39mreturn\u001b[39;00m client_update(optimizer, initial_model_weights, dataset)\n\u001b[1;32m    321\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m   \u001b[39m@computations\u001b[39;49m\u001b[39m.\u001b[39;49mtf_computation(weights_type, data_type)\n\u001b[0;32m--> 324\u001b[0m   \u001b[39mdef\u001b[39;49;00m \u001b[39mclient_update_computation\u001b[39;49m(initial_model_weights, dataset):\n\u001b[1;32m    325\u001b[0m     keras_optimizer \u001b[39m=\u001b[39;49m optimizer()\n\u001b[1;32m    326\u001b[0m     client_update \u001b[39m=\u001b[39;49m build_model_delta_update_with_keras_optimizer(\n\u001b[1;32m    327\u001b[0m         model_fn\u001b[39m=\u001b[39;49mmodel_fn,\n\u001b[1;32m    328\u001b[0m         weighting\u001b[39m=\u001b[39;49mclient_weighting,\n\u001b[1;32m    329\u001b[0m         delta_l2_regularizer\u001b[39m=\u001b[39;49mdelta_l2_regularizer,\n\u001b[1;32m    330\u001b[0m         use_experimental_simulation_loop\u001b[39m=\u001b[39;49muse_experimental_simulation_loop)\n",
      "File \u001b[0;32m~/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/wrappers/computation_wrapper.py:494\u001b[0m, in \u001b[0;36mComputationWrapper.__call__\u001b[0;34m(self, tff_internal_types, *args)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m   \u001b[39m# Either we have a concrete parameter type, or this is no-arg function.\u001b[39;00m\n\u001b[1;32m    493\u001b[0m   parameter_type \u001b[39m=\u001b[39m _parameter_type(parameters, parameter_types)\n\u001b[0;32m--> 494\u001b[0m   wrapped_func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy(\n\u001b[1;32m    495\u001b[0m       fn_to_wrap, fn_name, parameter_type, unpack\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    497\u001b[0m \u001b[39m# Copy the __doc__ attribute with the documentation in triple-quotes from\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[39m# the decorated function.\u001b[39;00m\n\u001b[1;32m    499\u001b[0m wrapped_func\u001b[39m.\u001b[39m\u001b[39m__doc__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(fn_to_wrap, \u001b[39m'\u001b[39m\u001b[39m__doc__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/wrappers/computation_wrapper.py:222\u001b[0m, in \u001b[0;36mPythonTracingStrategy.__call__\u001b[0;34m(self, fn_to_wrap, fn_name, parameter_type, unpack)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m   args, kwargs \u001b[39m=\u001b[39m unpack_arguments_fn(packed_args)\n\u001b[0;32m--> 222\u001b[0m   result \u001b[39m=\u001b[39m fn_to_wrap(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    223\u001b[0m   \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[39mraise\u001b[39;00m ComputationReturnedNoneError(fn_to_wrap)\n",
      "File \u001b[0;32m~/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py:331\u001b[0m, in \u001b[0;36mbuild_model_delta_client_work.<locals>.client_update_computation\u001b[0;34m(initial_model_weights, dataset)\u001b[0m\n\u001b[1;32m    325\u001b[0m keras_optimizer \u001b[39m=\u001b[39m optimizer()\n\u001b[1;32m    326\u001b[0m client_update \u001b[39m=\u001b[39m build_model_delta_update_with_keras_optimizer(\n\u001b[1;32m    327\u001b[0m     model_fn\u001b[39m=\u001b[39mmodel_fn,\n\u001b[1;32m    328\u001b[0m     weighting\u001b[39m=\u001b[39mclient_weighting,\n\u001b[1;32m    329\u001b[0m     delta_l2_regularizer\u001b[39m=\u001b[39mdelta_l2_regularizer,\n\u001b[1;32m    330\u001b[0m     use_experimental_simulation_loop\u001b[39m=\u001b[39muse_experimental_simulation_loop)\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m client_update(keras_optimizer, initial_model_weights, dataset)\n",
      "File \u001b[0;32m~/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file67xwzd92.py:84\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__client_update\u001b[0;34m(optimizer, initial_weights, data)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m     83\u001b[0m         \u001b[39mreturn\u001b[39;00m fscope_2\u001b[39m.\u001b[39mret(retval__2, do_return_2)\n\u001b[0;32m---> 84\u001b[0m num_examples \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(dataset_reduce_fn), (ag__\u001b[39m.\u001b[39mld(reduce_fn), ag__\u001b[39m.\u001b[39mld(data)), \u001b[39mdict\u001b[39m(initial_state_fn\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(initial_state_for_reduce_fn)), fscope)\n\u001b[1;32m     85\u001b[0m client_update \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure, (ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39msubtract, ag__\u001b[39m.\u001b[39mld(initial_weights)\u001b[39m.\u001b[39mtrainable, ag__\u001b[39m.\u001b[39mld(model_weights)\u001b[39m.\u001b[39mtrainable), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     86\u001b[0m model_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(model)\u001b[39m.\u001b[39mreport_local_unfinalized_metrics, (), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filel5pjm85q.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___dataset_reduce_fn\u001b[0;34m(reduce_fn, dataset, initial_state_fn)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(dataset)\u001b[39m.\u001b[39mreduce, (), \u001b[39mdict\u001b[39m(initial_state\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(initial_state_fn), (), \u001b[39mNone\u001b[39;00m, fscope), reduce_func\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(reduce_fn)), fscope)\n\u001b[1;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file67xwzd92.py:24\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__client_update.<locals>.reduce_fn\u001b[0;34m(num_examples_sum, batch)\u001b[0m\n\u001b[1;32m     22\u001b[0m retval__1 \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     23\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m---> 24\u001b[0m     output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(model)\u001b[39m.\u001b[39mforward_pass, (ag__\u001b[39m.\u001b[39mld(batch),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope_1)\n\u001b[1;32m     25\u001b[0m gradients \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tape)\u001b[39m.\u001b[39mgradient, (ag__\u001b[39m.\u001b[39mld(output)\u001b[39m.\u001b[39mloss, ag__\u001b[39m.\u001b[39mld(model_weights)\u001b[39m.\u001b[39mtrainable), \u001b[39mNone\u001b[39;00m, fscope_1)\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state\u001b[39m():\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileqfsybqrb.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__forward_pass\u001b[0;34m(self, batch_input, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_forward_pass, (ag__\u001b[39m.\u001b[39mld(batch_input),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filewzap42y5.py:40\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___forward_pass\u001b[0;34m(self, batch_input, training)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     39\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(inputs) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m, if_body_1, else_body_1, get_state_1, set_state_1, (), \u001b[39m0\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m predictions \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpredict_on_batch, (ag__\u001b[39m.\u001b[39mld(inputs), ag__\u001b[39m.\u001b[39mld(training)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_2\u001b[39m():\n\u001b[1;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m (y_true,)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filejzc8xc4i.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_on_batch\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_keras_model, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/input_spec.py:295\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m spec_dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     \u001b[39mif\u001b[39;00m spec_dim \u001b[39m!=\u001b[39m dim:\n\u001b[0;32m--> 295\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    297\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mincompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected shape=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound shape=\u001b[39m\u001b[39m{\u001b[39;00mdisplay_shape(x\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/learning/templates/model_delta_client_work.py\", line 179, in reduce_fn  *\n        output = model.forward_pass(batch, training=True)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/learning/framework/dataset_reduce.py\", line 33, in _dataset_reduce_fn  *\n        return dataset.reduce(initial_state=initial_state_fn(), reduce_func=reduce_fn)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 455, in forward_pass  *\n        return self._forward_pass(batch_input, training=training)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 408, in _forward_pass  *\n        predictions = self.predict_on_batch(inputs, training)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/tensorflow_federated/python/learning/keras_utils.py\", line 398, in predict_on_batch  *\n        return self._keras_model(x, training=training)\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/isbt/Desktop/wirluss/fed/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 10, 5, 1), found shape=(None, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "nPast = 10                  # Number of past samples used for predictions (equivalent to 20 minutes)\n",
    "nFuture = 1                 # Number of future samples to be predicted (equivalent to 2 minutes)\n",
    "windowSize = 1              # Window size (time-series forecasting)\n",
    "nEpochs = 100                # Number of epochs\n",
    "batch_size = 128            # Batch size (other options: [64, 32])\n",
    "nRep = 2  \n",
    "nVarsOut = y_train.shape[1]             # Output shape\n",
    "inputShape = nPast, x_train.shape[1], 1 # Input shape based on the number of past observations and the shape of the validation data\n",
    "\n",
    "\n",
    "# genValLosses = TimeseriesGenerator(x_val, y_val, nPast, stride=windowSize, batch_size=1)  # BS always 1 here!\n",
    "# genTrain = TimeseriesGenerator(x_train, y_train, nPast, stride=windowSize, batch_size=batch_size)\n",
    "# genVal = TimeseriesGenerator(x_val, y_val, nPast, stride=windowSize, batch_size=batch_size)\n",
    "\n",
    "def model_fn():\n",
    "  keras_model = create_keras_model(inputShape, nVarsOut, nFuture)\n",
    "  return tff.learning.from_keras_model(keras_model, input_spec=preprocessed_example_dataset.element_spec, loss = [tf.keras.losses.SparseCategoricalCrossentropy()])\n",
    "\n",
    "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
    "    model_fn,\n",
    "    client_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate = 0.02),\n",
    "    server_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate = 1),\n",
    "    )\n",
    "state = iterative_process.initialize()\n",
    "output = iterative_process.next(state, fed_train_data)\n",
    "state = output.state\n",
    "metrics = output.metrics\n",
    "print('round 1, metrics={}'.format(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "genTrain = TimeseriesGenerator(x_train, y_train, nPast, stride=windowSize, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43m(\u001b[49m\u001b[43mgenTrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "(genTrain.get_config()).shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tmp = pd.DataFrame.from_dict(genTrain)\n",
    "np.shape(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('fed': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31e2d1d5cfb83b98ad934fd33567af6c803d640f2ec0d15be4831a5eac94ddf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
